---
title: 'Cross-Domain Representation Learning for Clothes Unfolding in Robot-Assisted Dressing'

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here
# and it will be replaced with their full name and linked to their profile.
authors:
  - Jinge Qie
  - Yixing Gao
  - admin
  - Xin Wang
  - Jielong Yang
  - Esha Dasgupta
  - Hyung Jin Chang
  - Yi Chang

# Author notes (optional)
author_notes:
  - ''
  - ''
  - ''

date: '2022-07-30T00:00:00Z'
doi: ''

# Schedule page publish date (NOT publication's date).
publishDate: '2023-02-19T00:00:00Z'

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ['1']

# Publication name and optional abbreviated publication name.
publication: In *European Conference on Computer Vision Workshops (ECCV 2022 Workshops)*
publication_short: In *ECCV 2022 Workshops*

abstract: Assistive robots can significantly reduce the burden of daily activities by providing services such as unfolding clothes and dressing assistance. For robotic clothes manipulation tasks, grasping point recognition is one of the core steps, which is usually achieved by supervised deep learning methods using large amount of labeled training data. Given that collecting real annotated data is extremely labor-intensive and time-consuming in this field, synthetic data generated by physics engines is typically adopted for data enrichment. However, there exists an inherent discrepancy between real and synthetic domains. Therefore, effectively leveraging synthetic data together with real data to jointly train models for grasping point recognition is desirable. In this paper, we propose a Cross-Domain Representation Learning (CDRL) framework that adaptively extracts domain-specific features from synthetic and real domains respectively, before further fusing these domain-specific features to produce more informative and robust cross-domain representations, thereby improving the prediction accuracy of grasping points. Experimental results show that our CDRL framework is capable of recognizing grasping points more precisely compared with five baseline methods. Based on our CDRL framework, we enable a Baxter humanoid robot to unfold a hanging white coat with a 92% success rate and assist 6 users to dress successfully.

# Summary. An optional shortened abstract.
# summary: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt 

tags: []

# Display this page in the Featured widget?
featured: false

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

url_pdf: 'http://pure-oai.bham.ac.uk/ws/portalfiles/portal/176232895/027_source.pdf'
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
image:
  caption: ''
  focal_point: ''
  preview_only: false

---

